---
title: "Analysis of Patterns in Intention to Migrate in the 2018/19 AmericasBarometer Guatemala Survey with LASSO"
author: "Luke Plutowski"
date: 'July 2022'
output: html_document
---

```{r setup, include=FALSE, fig.align = "center"}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

# What is LASSO? 

LASSO (least absolute shrinkage and selection operator) is a regression _regularization_ technique, meaning a method to prevent overfitting of a model (which can reduce the accuracy of the model's predictions for new data).  Regularization is done by adding a penalty to the coefficients, shrinking them (the first "S" in LASSO) so that the predictions are not as sensitive to changes in the independent variables.  Whereas ordinary regression minimizes the residual sum of squares, 

$$
RSS = \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2
$$

where $y_i$ is the outcome variable value for observation $i$, $\beta_j$ is the coefficient for variable $j$, and $x_{ij}$ is the value of variable $j$ for observation $i$, the LASSO minimizes the loss function

$$
RSS + \lambda \sum_{j=1}^p |\beta_j|
$$
where $\lambda$ is the tuning parameter (the size of the penalty).  The "L" (least) in LASSO refers to the minimization of this function. Notice that the function uses the absolute values of the coefficients; this is the "A" (absolute) of LASSO.  Another regularization technique, ridge regression, instead uses the penalty term

$$
\lambda \sum_{j=1}^p \beta_j^2.
$$

Unlike ridge regression, which only shrinks coefficients _toward_ zero, LASSO may set some coefficients to zero, effectively dropping them from the model.  Because of this, LASSO can also be used as a variable selection method (the "SO" in LASSO stands for "selection operator").  Variable selection is used to make sure only relevant, useful independent variables are included, and ones that add little to the model's performance are dropped.  Other selection methods like forward or backward selection are computationally intensive compared to LASSO, since they require many models to be estimated in order to determine the optimal subset.   LASSO is especially useful when $p$, the number of predictors, is high.  

# Application to Guatemala 2018/19 Data

We are interested in predicting whether or not someone intends to migrate using the AmericasBarometer data.  We have a whole host of variables that may predict migration -- some that are strongly grounded in theory, and some that are speculative.  It is possible to throw all variables a "kitchen sink" regression model.  The problem with this approach, as alluded to above, is that the resultant model may overfit to the given data, perhaps resulting in the model assigning too much weight to particular variables.  This is especially the case when the number of observations is small, the number of predictors is high, and there is collinearity among the predictor variables.  All three of these conditions are likely true for the present analysis. 

LASSO can help us decide which of these variables are important enough to include in a model of migration intention.  Furthermore, if we apply a LASSO model to the subgroups identified by the clusters identified in the first edition of the migration analysis note, the LASSO can help us evaluate how predictors vary across "types" of migrants.  

For this analysis, I use data from the 2018/19 AmericasBarometer in Guatemala.  The outcome of interest, intention to migrate, is a binary variable drawn from the following question: 

- **Q14.** Do you have any intention of going to live or work in another country in the next three
years? (1) Yes (2) No 

The survey included several questions that probe attitudes which could be linked to intention to migrate.  Among them are questions related to:^[See full question wordings here: https://www.vanderbilt.edu/lapop/guatemala/ABGua18-v11.1.3.1-Spa-190117_W.pdf]

- **Economic hardship**: _soct2_ (sociotropic economic evaluation)
- **Community involvement**: _np1_ / _muni5_ (attendance at municipal meetings/participation in budget creation), _it1_ (interpersonal trust), _cp 6/7/8/13_ (attendance at religious/school/community improvement/political party meetings),  
- **Satisfaction with government**: _sgl1_ (satifaction with municipal services), _prot3_ (attendance at protest), _sd2new2_ / _sd3new2_ / _sd6new2_ (satsfaction with roads / schools / public health), _eff1/2_ (efficacy)
- **Crime**: _vic1ext_ (crime victimization), _vicbar7_ (murders in neighborhood), _vic41_ / _vic45n_ / _vicbar4a_ (changed behavior to avoid crime), _fear11_ / _aoj11_ (fear of crime), _pese1/2_ (evaulation of crime in own neighborhood)
- **Corruption**: _exc2/6_ (corruption victimization), _exc7_ (level of corruption among politicians) _corinv_ (punishment for corruption)
- **Disaster risk**: _drk1_

We can include all of these variables as predictors in LASSO models among the subgroups identified in an earlier analysis.  As a reminder, the groups are: 

- Group 1: Young professional men (men from urban areas who are single and under 40 years old)
- Group 2: Young professional women (women from urban areas who are single and under 40 years old)
- Group 3: Struggling older adults (from urban area, over 40 years old, have less than average income and education)
- Group 4: Middle class families (from urban area, not single and have average wealth)
- Group 5: The unemployed (unemployed)
- Group 6: Rural workers (from rural area, have job).  

For each feature (predictor variable), I use median imputation for missing values.  Because the outcome is binary, I estimate logistic regressions using all available variables.  The $\lambda$ (tuning parameter) for each model is unique and is chosen computationally by cross-validation (testing different values on subsets of the data to find the one that yields optimal accuracy).

```{r, results = 'hide'}
library(readstata13)
library(mice)
library(kableExtra)
library(glmnet)
library(stargazer)

gtm18 <- read.dta13("C:/Users/plutowl/Desktop/data/gtm_2019_cy_spa-eng_p_v1-0.dta", convert.factors = FALSE)

gtm18$migrate <- ifelse(gtm18$q14 == 2, 0, gtm18$q14)

gtm18$unemp <- ifelse(gtm18$ocup4a == 3 | gtm18$ocup4a == 7, 1,
                                 ifelse(gtm18$ocup4a < 3 | gtm18$ocup4a == 4 | gtm18$ocup4a == 5 | gtm18$ocup4a == 6, 0, NA))
gtm18$single <- ifelse(gtm18$q11n == 1 | gtm18$q11n == 4 | gtm18$q11n == 5 | gtm18$q11n == 6, 1,
                    ifelse(is.na(gtm18$q11n), NA, 0))

dem_vars <- c("q1", "q2" , "ur", "single", "ed", "q10new", "unemp")

tmp <- mice(gtm18[, dem_vars], m = 5, method = "pmm", maxit = 50, seed = 615)
complete <- complete(tmp,1)

gtm18$exc7b <- ifelse(is.na(gtm18$exc7), gtm18$exc7new, gtm18$exc7)

pred_vars <- c("soct2", "np1", "muni5", "it1", "cp6", "cp7", "cp8", "cp13", "sgl1",  "prot3", "sd2new2", "sd3new2", "sd6new2", "eff1", "eff2", "vic1ext", "vicbar7", "vic41", "fear11", "vic45n", "vicbar4a", "aoj11", "pese1", "pese2", "exc2", "exc6", "exc7b",  "corinv", "drk1")

gtm18preds <- gtm18[, pred_vars] 
# md.pattern(gtm18preds)
na_count <- sapply(gtm18preds, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)

col_medians <- apply(gtm18preds, 2, median, na.rm=TRUE)

# imputing median value with NA 
for(i in colnames(gtm18preds)){
  gtm18preds[,i][is.na(gtm18preds[,i])] = col_medians[i]
}

gtm18comp <- data.frame(cbind(gtm18$migrate, complete, gtm18preds))
gtm18comp <- gtm18comp[complete.cases(gtm18comp),]

c1 <- subset(gtm18comp, q1 == 1 & ur == 1 & q2 < 40 & single == 1)
c2 <- subset(gtm18comp, q1 == 2 & ur == 1 & q2 < 40 & single == 1)
c1b <- subset(gtm18comp,ur == 1 & q2 < 40 & single == 1)
c3 <- subset(gtm18comp, q2 > 40 & ed < 11 & q10new < 7)
c4 <- subset(gtm18comp, ur == 1 & single == 0 & q10new >= 3 & q10new <= 12)
c5 <- subset(gtm18comp, unemp == 1)
c6 <- subset(gtm18comp, ur == 2 & unemp == 0)
```


```{r, results = 'hide'}
y <- gtm18comp$gtm18.migrate
x <- data.matrix(gtm18comp[, -1])

cv_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")
best_lambda <- cv_model$lambda.min
best_lambda

# plot(cv_model)

best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda, family = "binomial")
coef(best_model)
```

```{r}
set.seed(615)

results <- matrix(0, nrow = length(pred_vars) + 1, ncol = 6)

y <- c1$gtm18.migrate
x <- data.matrix(c1[, pred_vars])

cv_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")
best_lambda <- cv_model$lambda.min

best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda, family = "binomial")
results1 <- coef(best_model)


y <- c2$gtm18.migrate
x <- data.matrix(c2[, pred_vars])

cv_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")
best_lambda <- cv_model$lambda.min

best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda, family = "binomial")
results2 <- coef(best_model)

y <- c3$gtm18.migrate
x <- data.matrix(c3[, pred_vars])

cv_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")
best_lambda <- cv_model$lambda.min

best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda, family = "binomial")
results3 <- coef(best_model)

y <- c4$gtm18.migrate
x <- data.matrix(c4[, pred_vars])

cv_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")
best_lambda <- cv_model$lambda.min

best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda, family = "binomial")
results4 <- coef(best_model)


y <- c5$gtm18.migrate
x <- data.matrix(c5[, pred_vars])

cv_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")
best_lambda <- cv_model$lambda.min

best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda, family = "binomial")
results5 <- coef(best_model)


y <- c6$gtm18.migrate
x <- data.matrix(c6[, pred_vars])

cv_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")
best_lambda <- cv_model$lambda.min

best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda, family = "binomial")
results6 <- coef(best_model)

y <- c1b$gtm18.migrate
x <- data.matrix(c1b[, pred_vars])
```

<center>

```{r}
results <- cbind(results1, results2, results3, results4, results5, results6)
resultsdf <- as.data.frame(as.matrix(results))
resultsdf[resultsdf==0] <- NA

names(resultsdf) <- c("Group 1", "Group 2", "Group 3", "Group 4", "Group 5", "Group 6")
options(knitr.kable.NA = '')

kbl(resultsdf, 
    caption = "LASSO Coefficients Predicting Intention to Migrate",
    digits = 3) %>%
    kable_styling(bootstrap_options = "striped", font_size = 10,
                  full_width = FALSE)

```

</center>

The table above shows the results of the LASSO regression analysis.  Empty cells indicate that the coefficient was reduced to zero.  As can be observed from the table, there is a high degree of variability in which variables were dropped across the groups. 

In groups 1 & 2, all coefficients were shrunk to zero. For this particular subset of data, value of $\lambda$, and set of predictors, no combination of variables offers a useful prediction of whether or not someone will migrate with a penalty added.  It may be that for young professionals, the decision to migrate or not is mostly economic (which we do not have much data on here) and unrelated to extraneous factors like crime, corruption, and disaster risk.  Groups 3 and 5 (struggling older adults and the unemployed) had the fewest variables dropped from the model, while group 4 (middle class families) had just five non-zero coefficients: cp13 (attendance at political party meetings), sgl1 (satisfaction with municipal government services), prot3 (attendance at protests), vic45n (changed place of work for fear of crime), and exc2 (asked for bribe by police).  

In all groups besides 1 and 2, vic45n (has R changed place of work or study) is positively related to intention to migrate.  Corruption victimization by police (exc2) is predicts intention to migrate among groups 3-6, but corruption by other public officials does not.  Disaster risk (drk1) is dropped from all models, as are cp6 (religious meeting attendance), np1 (attendance at municipal sessions), sd2new2 and sd6new (satisfaction with roads and public health), vic41 (avoided recreations due to fear of crime), aoj11 (security of neighborhood), pese1 (security of neighborhood compared to others), corinv (whether corrupt officials are brought to justice).  These factors could be important on their own but are perhaps subsumed by other similar-looking variables.  

Group 4 (middle class families) seem to be driven to migrate mostly by political variables.  Interestingly, satisfaction with municipal government services is negatively related to intention to migrate among that group, but positively related for group 5.  Groups 3 (low SES, older) and 5 (unemployed) are affected by a whole host of factors; perhaps these groups are too broad or diverse to have a common set of behavioral or attitudinal drivers.  Group 6 (rural, employed) appear to be influenced by push factors including crime and corruption, but are also constrained by pull factors including attendance at community meetings.  

We can compare these results to those obtained by regular logistic regression.  The table below shows the results of "kitchen sink" models for each group.  Note that the observations in each group are quite low compared to the number of predictors, which generally leads to poor model performance.  

<center>

```{r, results='hide'}
c1reg <- glm(gtm18.migrate ~ . - q1 - q2 - ur - single - ed - q10new -unemp, data = c1, family = "binomial")
c2reg <- glm(gtm18.migrate ~ . - q1 - q2 - ur - single - ed - q10new -unemp, data = c2, family = "binomial")
c3reg <- glm(gtm18.migrate ~ . - q1 - q2 - ur - single - ed - q10new -unemp, data = c3, family = "binomial")
c4reg <- glm(gtm18.migrate ~ . - q1 - q2 - ur - single - ed - q10new -unemp, data = c4, family = "binomial")
c5reg <- glm(gtm18.migrate ~ . - q1 - q2 - ur - single - ed - q10new -unemp, data = c5, family = "binomial")
c6reg <- glm(gtm18.migrate ~ . - q1 - q2 - ur - single - ed - q10new -unemp, data = c6, family = "binomial")

stargazer(c1reg, c2reg, c3reg, c4reg, c5reg, c6reg, 
          type = "html",
          title = "Logistic Regression Predicting Intention to Migrate",
          dep.var.labels = "Intent to Migrate (1 = yes)",
          colnames = FALSE,
          model.numbers = FALSE,
          column.labels = c("Group 1", "Group 2", "Group 3", "Group 4", "Group 5", "Group 6"),
          se = NULL,
          digits = 2, 
          report = "vc*", 
          out = "table2.html")
```

```{r, echo=FALSE}
htmltools::includeHTML("table2.html")
```

</center>

Comparing results from the two tables, we see generally overlapping patterns between the LASSO and logistic models.  Coefficients which are zeroed-out by the LASSO are not significant predictors in the regular logistic models, and the significant predictors in the logistic models are not zero in the LASSOs.  However, the logistic regressions are more strict, in the sense that they have fewer significant coefficients that LASSO has non-zero coefficients.  However, 

As in the LASSO, intention to migrate among group 1 is difficult to predict, having no significant coefficients at the $p < 0.05$ level.  For group 2, eff1 and pese2 emerge as significant predictors.  For group 3, it1, cp8, sgl1, eff1, and fear11 are associated with intention to migrate.  Only prot3, vic45n, and exc2 are significantly related to migration intention in group 4.  Cp8, sgl1, and vic45n are significant for group 5.  Finally, in group 6, sd3new, eff1, and vic1ext are significant. 

Compared to table 1, table 2 tells a stronger story about eff1, external political efficacy.  This is a significant covariate at $p < 0.1$ in groups 2, 3, 5, and 6.  Both approaches show that vic45n, whether someone has changed jobs for fear of crime, is quite important for predicting migration among groups 4 and 5. Cp8, attendance at meetings for community improvement, is negatively associated with intention to migrate for groups 3 and 5 in both models.  The LASSO model appears to show that group 4 is mostly influenced by political variables, but the logistic regression indicates that they are highly affected by crime and corruption as well.  In both approaches, group 1 is difficult to predict, but the logistic model for group 2 indicates significant predictors of migration.    

This exercise shows one approach to analyzing the factors that cause or prevent migration, and how those factors vary by subgroup.  The use of the LASSO as a variable selection method allows us to sort through the multitude of available data to pinpoint the most important variables when it comes to assessing migration risk.  
